{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import code from file in upper directory\n",
    "import sys, os\n",
    "sys.path.append(os.getcwd() + os.sep + os.pardir)\n",
    "from tweet_data import TweetsBaseDataset, TweetsBOWDataset\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will examine the effect of preprocessing on model performance, by training a classifier with data processed by different preprocessing functions. In each experiment the preprocessing function changes, and the F1 scores on the training and dev sets are reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(process_fn):\n",
    "    def score_model(model, dataset):\n",
    "        predictions = model.predict(dataset.data)\n",
    "        return f1_score(dataset.labels, predictions, average='macro')\n",
    "        \n",
    "    # Overwrite processing function\n",
    "    TweetsBaseDataset.process_tweet = process_fn\n",
    "    \n",
    "    # Load and preprocess datasets with new processing function\n",
    "    train_set = TweetsBOWDataset('../data/train', 'us_train')\n",
    "    dev_set = TweetsBOWDataset('../data/dev', 'us_trial', vocabulary=train_set.vocabulary)\n",
    "    \n",
    "    # Train and score model on train and dev sets\n",
    "    model = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "    model.fit(train_set.data, train_set.labels)\n",
    "    \n",
    "    train_score = score_model(model, train_set)\n",
    "    dev_score = score_model(model, dev_set)\n",
    "    \n",
    "    return train_score, dev_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get first the results on the default processing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file\n",
      "Read file with 489609 tweets\n",
      "Building vocabulary\n",
      "Loading labels\n",
      "Loading counts matrix\n",
      "Creating TF-ID matrix\n",
      "Reading file\n",
      "Read file with 50000 tweets\n",
      "Using vocabulary containing 10002 tokens\n",
      "Loading labels\n",
      "Loading counts matrix\n",
      "Creating TF-ID matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2768806185593461, 0.23972266714875984)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment(TweetsBaseDataset.process_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file\n",
      "Read file with 489609 tweets\n",
      "Building vocabulary\n",
      "Loading labels\n",
      "Loading counts matrix\n",
      "Creating TF-ID matrix\n",
      "Reading file\n",
      "Read file with 50000 tweets\n",
      "Using vocabulary containing 10002 tokens\n",
      "Loading labels\n",
      "Loading counts matrix\n",
      "Creating TF-ID matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.28008270108736233, 0.23821311615325777)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def process_tweet(self, text):\n",
    "    \"\"\" Process and tokenize a tweet.\n",
    "    Args:\n",
    "        - text (str): a raw tweet in string format\n",
    "    Returns: list, containing tokens after processing\n",
    "    \"\"\"\n",
    "    \n",
    "    return nltk.word_tokenize(text.lower())\n",
    "\n",
    "run_experiment(process_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file\n",
      "Read file with 489609 tweets\n",
      "Building vocabulary\n",
      "Loading labels\n",
      "Loading counts matrix\n",
      "Creating TF-ID matrix\n",
      "Reading file\n",
      "Read file with 50000 tweets\n",
      "Using vocabulary containing 10002 tokens\n",
      "Loading labels\n",
      "Loading counts matrix\n",
      "Creating TF-ID matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.280172011524204, 0.23679280728802513)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def process_tweet(self, text):\n",
    "    \"\"\" Process and tokenize a tweet.\n",
    "    Args:\n",
    "        - text (str): a raw tweet in string format\n",
    "    Returns: list, containing tokens after processing\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "run_experiment(process_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file\n",
      "Read file with 489609 tweets\n",
      "Building vocabulary\n",
      "Loading labels\n",
      "Loading counts matrix\n",
      "Creating TF-ID matrix\n",
      "Reading file\n",
      "Read file with 50000 tweets\n",
      "Using vocabulary containing 10002 tokens\n",
      "Loading labels\n",
      "Loading counts matrix\n",
      "Creating TF-ID matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2777279092004158, 0.23426146648482332)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def process_tweet(self, text):\n",
    "    \"\"\" Process and tokenize a tweet.\n",
    "    Args:\n",
    "        - text (str): a raw tweet in string format\n",
    "    Returns: list, containing tokens after processing\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "run_experiment(process_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ekphrasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n",
      "Reading file\n",
      "Read file with 489609 tweets\n",
      "Building vocabulary\n",
      "Loading labels\n",
      "Loading counts matrix\n",
      "Creating TF-ID matrix\n",
      "Reading file\n",
      "Read file with 50000 tweets\n",
      "Using vocabulary containing 10002 tokens\n",
      "Loading labels\n",
      "Loading counts matrix\n",
      "Creating TF-ID matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.287221018371828, 0.2446901329507131)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis'},\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize\n",
    ")\n",
    "\n",
    "def process_tweet(self, text):\n",
    "    \"\"\" Process and tokenize a tweet.\n",
    "    Args:\n",
    "        - text (str): a raw tweet in string format\n",
    "    Returns: list, containing tokens after processing\n",
    "    \"\"\"\n",
    "    return text_processor.pre_process_doc(text)\n",
    "\n",
    "run_experiment(process_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Method        | Train Score | Dev Score  |\n",
    "| ------------- | -----------:| ----------:|\n",
    "| Tokenization  | 0.2801      | 0.2382     |\n",
    "| Stemming      | 0.2802      | 0.2368     |\n",
    "| Lemmatizing   | 0.2777      | 0.2343     |\n",
    "| Ekphrasis     | **0.2872**  | **0.2447** |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
