{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import code from file in upper directory\n",
    "import sys, os\n",
    "sys.path.append(os.getcwd() + os.sep + os.pardir)\n",
    "from tweet_data import TweetsBaseDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the dev dataset for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file\n",
      "Read file with 50000 tweets, 77053 unique tokens\n",
      "Building vocabulary\n",
      "Loading labels\n"
     ]
    }
   ],
   "source": [
    "dataset = TweetsBaseDataset('../data/dev', 'us_trial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to get batches of variable-length sequences we have to write our custom `collate_fn` for the `DataLoader`. This function is defined in `TweetBaseDataset.collate_fn()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data_loader = DataLoader(dataset, collate_fn=TweetsBaseDataset.collate_fn, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the data loader to get batches of the data. Each batch contains the padded sequences, the labels, and the length of each sequence. Padding is inserted with zeros (consistent with `dataset.vocabulary['<PAD>']`, which maps to 0) and sequences are sorted from longest to shortest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequences:\n",
      " tensor([[  73,    1,  240,   74],\n",
      "        [  16,   57,   18,    1],\n",
      "        [ 176,  192,    1,    3],\n",
      "        [ 167,    1,    1,   22],\n",
      "        [  12,   10,    5,    1],\n",
      "        [  42,  625,  871,    0],\n",
      "        [  15,  149,   17,    0],\n",
      "        [   9,  711,    7,    0],\n",
      "        [ 438,  235, 5753,    0],\n",
      "        [ 187,  133,    3,    0],\n",
      "        [  81, 8899, 6441,    0],\n",
      "        [   1,    1, 3219,    0],\n",
      "        [  69,    3, 1517,    0],\n",
      "        [ 378, 1996,    1,    0],\n",
      "        [   4,    5,    0,    0],\n",
      "        [   3,  662,    0,    0],\n",
      "        [   1,    0,    0,    0],\n",
      "        [   5,    0,    0,    0],\n",
      "        [ 273,    0,    0,    0]])\n",
      "Labels:\n",
      " tensor([ 0, 10,  1,  1])\n",
      "Sequence lenghts:\n",
      " [19 16 14  5]\n"
     ]
    }
   ],
   "source": [
    "data, labels, lengths = next(iter(data_loader))\n",
    "print('Padded sequences:\\n', data)\n",
    "print('Labels:\\n', labels)\n",
    "print('Sequence lenghts:\\n', lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence lengths can be used to create a `PackedSequence`, which avoids calculating the output of recurrent models for padding tokens. A `PackedSequence` is created using `pack_padded_sequence()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2585, -0.1482, -0.2173,  0.2026, -0.1283, -0.0888],\n",
      "         [ 0.3145,  0.4838, -0.2319,  0.3397,  0.0161,  0.0036],\n",
      "         [ 0.3235,  0.0585,  0.2628,  0.2954,  0.1079,  0.0250],\n",
      "         [ 0.3393, -0.0997, -0.0520,  0.1779,  0.3314, -0.0976]]],\n",
      "       grad_fn=<ThAddBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "# Model definition\n",
    "embedding_dim = 100\n",
    "embeddings = torch.nn.Embedding(len(dataset.vocabulary), embedding_dim)\n",
    "rnn = torch.nn.RNN(embedding_dim, embedding_dim)\n",
    "linear = torch.nn.Linear(embedding_dim, 6)\n",
    "\n",
    "# Forward pass with padded batch of data\n",
    "def example_forward(data, lengths):\n",
    "    x = embeddings(data)\n",
    "    x = pack_padded_sequence(x, lengths)\n",
    "    _, x = rnn(x)\n",
    "    x = linear(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "print(example_forward(data, lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that throughout these examples we have been using the default setting in PyTorch where the first axis corresponds to the sequence, and the second corresponds to batches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
